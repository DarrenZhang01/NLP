{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy and Decisions \n",
    "### Material from CSC401, prof Frank Rudzicz, University of Toronto - http://www.cs.toronto.edu/~frank/csc401/lectures2018/3_Entropy_decisions.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Entropy</b>: The average amount of information we get in observing the output of source $S$:\n",
    "\n",
    "$$\n",
    "H(S) = \\displaystyle \\sum_{i}p_iI(w_i) = \\displaystyle \\sum_{i}p_i\\log_2 \\frac{1}{p_i}\n",
    "$$\n",
    "\n",
    "It is very similar to how we define the expected value of something:\n",
    "\n",
    "$$\n",
    "E[X] = \\displaystyle \\sum_{x\\in X}^{}p(x)x\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Flatter distribtuions_** have a **_higher_** entropy because the choices are more equivalent, on average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### $\\bullet$ Bounds on Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Maximum:**  (Uniform distribution $S_1$, since at this time, the distribution is flattest and thus gives highest surprise and uncertainty)\n",
    "Given $M$ choices,\n",
    "\n",
    "$$\n",
    "H(S_1) = \\displaystyle \\sum_{i}p_i \\log_2 \\frac{1}{p_i} = \\displaystyle \\sum_{i}\\frac{1}{M} \\log_2 M = \\log_2 M\n",
    "$$\n",
    "\n",
    "**Minimum:** only one choice, \n",
    "    \n",
    "$$\n",
    "H(S_2) = p_i\\log_2 \\frac{1}{p_i} = 1 \\log_2 1 = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bullet$ **_Alternative notions of entropy:_**\n",
    "\n",
    "a. The **average** amount of information provided by symbols in a vocabulary.\n",
    "\n",
    "b. The **average** amount of uncertainty you have before observing a symbol from a vocabulary.\n",
    "\n",
    "c. The **average** amount of surprise you receive when observing a symbol.\n",
    "\n",
    "d. The number of bits needed to communicate that alphabet.\n",
    "\n",
    "   (Shannon showed that you **cannot** have a coding scheme that can communicate the vocabulary more efficiently than $H(S)$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### $\\bullet$ Joint Entropy\n",
    " \n",
    " The **average** amount of information needed to specify **multiple** variables **simutaneouly**:\n",
    " \n",
    " $$\n",
    " H(X, Y) = \\displaystyle \\sum_{x}\\sum_{y}p(x, y)\\log_2 \\frac{1}{p(x, y)}\n",
    " $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### $\\bullet$ Conditional Entropy\n",
    " \n",
    " The average amount of information needed to specify one variable given that you know another (\"equivocation\")\n",
    " \n",
    " $$\n",
    " H(Y|X) = \\displaystyle \\sum_{x\\in X} p(x)H(Y|X=x)\n",
    " $$\n",
    " \n",
    " (**_Note_**: **Equivocation removes uncertainty** e.g. Entropy(i.e. confusion) about temperature is reduced if we know how wet it is outside.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### $\\bullet$ Mutual Information\n",
    " \n",
    " The average amount of information shared between variables\n",
    " \n",
    " $$\n",
    " I(X;Y) = \\displaystyle \\sum_{x, y}p(x, y)\\log_2\\frac{p(x, y)}{p(x)p(y)} = H(Y) - H(Y|X) \n",
    " = H(X) - H(X|Y)\n",
    " $$\n",
    " \n",
    " It represents:<br/>\n",
    " $\\bullet$ The amount of uncertainty removed in variable $X$ if you know $Y$<br/>\n",
    " $\\bullet$ The amount of uncertainty removed in variable $Y$ if you know $X$\n",
    " \n",
    " **Note:** When $X$ is independent of $Y$, \n",
    " \n",
    " $$\n",
    "  I(X;Y) = \\displaystyle \\sum_{x, y}p(x, y)\\log_2\\frac{p(x, y)}{p(x)p(y)} = \\displaystyle \\sum_{x, y}p(x, y)\\log_2\\frac{p(x)p(y)}{p(x)p(y)} = 0\n",
    " $$\n",
    " \n",
    " where there is **no** mutual information between $X$ and $Y$ in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### $\\bullet$ Relations between entropies:\n",
    " \n",
    " $$\n",
    " H(X,Y) = H(X) + H(Y) - I(X;Y) \n",
    " $$\n",
    " \n",
    " $$\n",
    " H(X, Y)= H(X|Y) + H(Y|X) + I(X;Y) \n",
    " $$\n",
    " \n",
    " $$\n",
    " H(X, Y)= H(X|Y) + H(Y)\n",
    " $$\n",
    " \n",
    " $$\n",
    "  H(X, Y)= H(Y|X) + H(X)\n",
    " $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### $\\bullet$ Relating Corpora\n",
    " \n",
    "<h4> Kullback-Leibler divergence: </h4> \n",
    "\n",
    "The average log difference between the distributions $P$ and $Q$, relative to $Q$ (a.k.a. **_relative entropy_**)\n",
    "\n",
    "$$\n",
    "D_{KL}(P||Q) = \\displaystyle \\sum_{i} P(i)\\log \\frac{P(i)}{Q(i)}\n",
    "$$\n",
    "\n",
    "(**Note**: $D_{KL}(P||Q) \\geqslant 0 , \\forall P, Q $ with $\"=\"$ if and only if $P$ and $Q$ are identical )\n",
    "\n",
    "KL divergence is not symmetric, $D_{KL}(P||Q)\\neq D_{KL}(Q||P)$\n",
    "\n",
    "Also,\n",
    "\n",
    "$$\n",
    "I(P; Q) = D_{KL}(P(X, Y)||P(X)P(Y))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### $\\bullet$ Entropy as intrinsic LM evaluation\n",
    " \n",
    " **Cross-entropy** measures how difficult it is to encode an event drawn from a **_true probability_** $p$ given a **_model_** based on a distribution $q$.\n",
    " \n",
    " If we do not know the **_true probability_** $p$, we need to <br/>$\\quad$ a. **_estimate p_**  <br/>$\\quad$ b. estimate $p$ by  estimating the **_probability of a test corpus_** $C$ using the distribution $q$:\n",
    " \n",
    " $$\n",
    " P_q(C)\n",
    " $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### $\\bullet$ Calculate the probability of a corpus\n",
    " \n",
    "For a **sentence**,\n",
    "\n",
    "$$\n",
    "P(s_i) = P(w_1)\\displaystyle \\prod_{t=2}^{n} P(w_t|w_{1:(t-1)})\n",
    "$$\n",
    "\n",
    "Its approximation:\n",
    "\n",
    "$$\n",
    "P(s_i) \\approx \\displaystyle \\prod_{t=1}^{n}P(w_t)\n",
    "$$\n",
    "\n",
    "For a **Corpus**,\n",
    "\n",
    "$$\n",
    "P(C) = P(w_1)\\displaystyle \\prod_{t=2}^{||C||}P(w_t|w_{1:(t-1)})\n",
    "$$\n",
    "\n",
    "Its approximation:\n",
    "\n",
    "$$\n",
    "P(C) \\approx \\displaystyle \\prod_{i}P(s_i)\n",
    "$$\n",
    "\n",
    "Regardless of the LM used for $P(s_i)$, we can assume **complete independence** between sentences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
