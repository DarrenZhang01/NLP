{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy and Decisions \n",
    "### Material from CSC401, prof Frank Rudzicz, University of Toronto - http://www.cs.toronto.edu/~frank/csc401/lectures2018/3_Entropy_decisions.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Entropy</b>: The average amount of information we get in observing the output of source $S$:\n",
    "\n",
    "$$\n",
    "H(S) = \\displaystyle \\sum_{i}p_iI(w_i) = \\displaystyle \\sum_{i}p_i\\log_2 \\frac{1}{p_i}\n",
    "$$\n",
    "\n",
    "It is very similar to how we define the expected value of something:\n",
    "\n",
    "$$\n",
    "E[X] = \\displaystyle \\sum_{x\\in X}^{}p(x)x\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Flatter distribtuions_** have a **_higher_** entropy because the choices are more equivalent, on average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### $\\bullet$ Bounds on Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Maximum:**  (Uniform distribution $S_1$, since at this time, the distribution is flattest and thus gives highest surprise and uncertainty)\n",
    "Given $M$ choices,\n",
    "\n",
    "$$\n",
    "H(S_1) = \\displaystyle \\sum_{i}p_i \\log_2 \\frac{1}{p_i} = \\displaystyle \\sum_{i}\\frac{1}{M} \\log_2 M = \\log_2 M\n",
    "$$\n",
    "\n",
    "**Minimum:** only one choice, \n",
    "    \n",
    "$$\n",
    "H(S_2) = p_i\\log_2 \\frac{1}{p_i} = 1 \\log_2 1 = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bullet$ **_Alternative notions of entropy:_**\n",
    "\n",
    "a. The **average** amount of information provided by symbols in a vocabulary.\n",
    "\n",
    "b. The **average** amount of uncertainty you have before observing a symbol from a vocabulary.\n",
    "\n",
    "c. The **average** amount of surprise you receive when observing a symbol.\n",
    "\n",
    "d. The number of bits needed to communicate that alphabet.\n",
    "\n",
    "   (Shannon showed that you **cannot** have a coding scheme that can communicate the vocabulary more efficiently than $H(S)$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### $\\bullet$ Joint Entropy\n",
    " \n",
    " The **average** amount of information needed to specify **multiple** variables **simutaneouly**:\n",
    " \n",
    " $$\n",
    " H(X, Y) = \\displaystyle \\sum_{x}\\sum_{y}p(x, y)\\log_2 \\frac{1}{p(x, y)}\n",
    " $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### $\\bullet$ Conditional Entropy\n",
    " \n",
    " The average amount of information needed to specify one variable given that you know another (\"equivocation\")\n",
    " \n",
    " $$\n",
    " H(Y|X) = \\displaystyle \\sum_{x\\in X} p(x)H(Y|X=x)\n",
    " $$\n",
    " \n",
    " (**_Note_**: **Equivocation removes uncertainty** e.g. Entropy(i.e. confusion) about temperature is reduced if we know how wet it is outside.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
