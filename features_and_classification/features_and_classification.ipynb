{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features and Classification\n",
    "\n",
    "### Materials from CSC401, prof Frank Rudzicz, University of Toronto -\n",
    "\n",
    "### http://www.cs.toronto.edu/~frank/csc401/lectures2018/4_Features_classification.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\bullet$ Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\bullet$ Feature:\n",
    "\n",
    "A measurable **variable** that is (rather, should be) **distinctive** of something we want to model.\n",
    "\n",
    "We usually choose features that are useful to **identify** something, i.e. to do **_classification_**.\n",
    "\n",
    "#### $\\bullet$ Feature Vector: \n",
    "\n",
    "Values for several features of an observation can be put into a single vector.\n",
    "\n",
    "Feature Vectors should be useful in **_discriminating_** between categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\bullet$ Preprocessing:\n",
    "\n",
    "Preprocessing involves **preparing** your data to make feature extraction easier or more valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bullet$ **Features in Sentiment Analysis:**\n",
    "\n",
    "Sentiment Analysis can involve detecting:\n",
    "\n",
    "> **Stress** or __Frustration__ in a conversation.\n",
    "\n",
    ">  __Interests__, __Confusion__ or __preferences__. Useful to marketers.\n",
    "\n",
    ">  __Lies__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\bullet$ Parts of Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bullet$ Linguists like to group words according to their **structural function** in building sentences.\n",
    "\n",
    "$\\bullet$ **Part-of-speech:** lexical category or morphological class.\n",
    "> (e.g. Nouns collectively constiture a part of speech, called _Nouns_)\n",
    "\n",
    "> (other examples: Verb, Adjective, Adverb, Preposition, Pronoun, Determiner, Conjunction, Particles, Auxiliaries, Numerals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bullet$ **Contentful Parts-of-Speech**\n",
    "\n",
    "**Contentful PoS** usually contain more words:\n",
    "\n",
    "e.g. an _app_, to _google_ ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bullet$ **Functional Parts-of-Speech**\n",
    "\n",
    "> **Functional** PoS usually cover a small and fixed number of word types (i.e. a **closed** class)\n",
    "\n",
    "> Their semantics depend on the contentful words with which they are used (i.e. _I' am __on__ time_, _I am __on__ a robot_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bullet$ **Grammatical Features**\n",
    "\n",
    "There are several grammatical features that can be associated with words:\n",
    "\n",
    "> Case \n",
    "\n",
    "> Person\n",
    "\n",
    "> Nnumber\n",
    "\n",
    "> Gender\n",
    "\n",
    "These features can **restrict** other words in a sentence.\n",
    "\n",
    "$\\bullet$ **Grammatical Features - Case:** The grammatical form of a noun or a pronoun\n",
    "\n",
    "> Nominative: the **subject** of a verb\n",
    "\n",
    "> Accusative: the **direct object** of a verb\n",
    "\n",
    "> Dative: the **indirect** object of a verb\n",
    "\n",
    "> Genitive: indicates **posession** (e.g. your **mom**'s book)\n",
    "\n",
    "$\\bullet$ **Grammatical Features - Person:** First, Second, Third\n",
    "\n",
    "$\\bullet$ **Grammatical Features - Number:** broad numerical distinction\n",
    "\n",
    "$\\bullet$ **Grammatical Features - Gender:** typically partitions **nouns** into classes associated with biological gender. **Not** typical in English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bullet$ **Agreement**\n",
    "\n",
    "> Parts-of-Speech should match in certain ways.\n",
    "\n",
    "> **Articles** have to agree with the **number** of their **noun**.\n",
    "\n",
    "> **Verbs** have to agree with their **subject**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bullet$ **PoS tagging**\n",
    "\n",
    "**Tagging**: the process of **assigning** a **part-of-speech** to each word in a sequence.\n",
    "\n",
    "$\\bullet$ The use of tagging:\n",
    "\n",
    "> **Speech Synthesis:** how to pronounce text (the same word may have different pronunciation in different parts of speech)\n",
    "\n",
    "> **Information Extraction:** quickly find names and relations\n",
    "\n",
    "> **Machines Translation:** Identifying grammaticall chunks is useful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bullet$ **Tagging as Classification**\n",
    "\n",
    "We have access to a **sequence of observations** and are expected to decide on the best assignment of a **hidden variable**, i.e. the PoS\n",
    "\n",
    "$\\bullet$ **Rule-based Tagging**\n",
    "\n",
    "> 1. Start with a dictionary\n",
    "\n",
    "> 2. Assign all possible tags to words from that dictionary\n",
    "\n",
    "> 3. Write rules to selectively remove tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bullet$ **Statistical PoS tagging**\n",
    "\n",
    "Determine the **most-likely** tag sequence $t_{1:n}$ by:\n",
    "\n",
    "$$\n",
    "\\underset{t_{1:n}}{\\operatorname{argmax}} P(t_{1:n}|w_{1:n}) = \\underset{t_{1:n}}{\\operatorname{argmax}} P(w_{1:n}|t_{1:n})P(t_{1:n}) \\text{         (Using Baye's Rule)}\n",
    "$$ \n",
    "\n",
    "Thus, \n",
    "\n",
    "$$\n",
    "\\underset{t_{1:n}}{\\operatorname{argmax}} P(t_{1:n}|w_{1:n}) \\approx \\underset{t_{1:n}}{\\operatorname{argmax}} \\displaystyle \\prod_{i}^{n} P(w_i|t_i)P(t_i|t_{i-1}) \\text{        (Assuming independence and assuming Markov)}\n",
    "$$ \n",
    "\n",
    "(Note: here the equation is simplified under Markov Assumption, which means that the current tag only depends on the previous tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\bullet$ Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General Process**:\n",
    "\n",
    "> We gather a big and relevant **training** corpus.\n",
    "\n",
    "> We learn our **parameters** (e.g. probabilities) from that corpus to build our **model**.\n",
    "\n",
    "> Once that model is fixed, we use those probabilities to evaluate **testing** data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bullet$ **K-fold Cross Validation**\n",
    "\n",
    "splitting all the data into $K$ **partitions** and iteratively testing on each after training on the rest (report means and variances)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bullet$ **Types of Classifiers**:\n",
    "\n",
    "Generative Classifiers model the world:\n",
    "\n",
    "> Parameters set to maximize likelihood of training data.\n",
    "\n",
    "> We can generate new observations from these (e.g. hidden Markov models)\n",
    "\n",
    "Discriminative classifiers emphasize class boudaries:\n",
    "\n",
    "> Parameters set to minimize error on training data. (e.g. support vector machines, decision trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bullet$ **Kernel Trick**\n",
    "\n",
    "We can sometimes linearize a non-linear case by moving the data into a higher dimension with a **kernel function**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bullet$ **Support Vector Machines (SVM)**\n",
    "\n",
    "The **margin** is the width by which the boundary could be **increased** before it hits a training datum.\n",
    "\n",
    "> The **maximum margin linear classifier** is the linear classifier with the maximum margin.\n",
    "\n",
    "> The **support vectors** are those data points against which the margin is pressed.\n",
    "\n",
    "> The bigger the margin, the less sensitive the boundary is to error.\n",
    "\n",
    "*The maximum margin helps SVM **generalize** to situations where it is **impossible** to linearly seperate the data.*\n",
    "\n",
    "We simutaneously:\n",
    "\n",
    "> **maximize the margin**\n",
    "\n",
    "> **minimize the misclassification error**\n",
    "\n",
    "(There is a straightforward approach to solving this system based on **quadratic programming**)\n",
    "\n",
    "$\\bullet$ SVMs are empirically very accurate classifiers\n",
    " \n",
    "> They perform well in situations where data are **static** (i.e. do not change over time)\n",
    "\n",
    "> SVMs do not generalize as well to **time-variant** systems (Kernel functions tend to not allow for observations of **different lengths**, i.e. all data points have to be of the same dimensionality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bullet$ **Decision Trees**\n",
    "\n",
    "Consists of **rules** for classifying data that have many **attributes**(features)\n",
    "\n",
    "> **Decision Nodes**: **Non-terminal**: Consists of a question asked of one of the attributes, and a branch for each possible answer.\n",
    "\n",
    "> **Leaf Nodes**: **Terminal**: Consist of a single class/category, so no further testing is required.\n",
    "\n",
    "**ID3** is an algorithm invented by Ross Quinlan to produce decision trees from data, which is: \n",
    "\n",
    "> 1. Compute the entropy of asking about each attribute.\n",
    "\n",
    "> 2. Choose the attribute which reduces the most entropy.\n",
    "\n",
    "> 3. Make a node asking a question of that attribute. \n",
    "\n",
    "> 4. Go to step 1, minus the chosen attribute."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
